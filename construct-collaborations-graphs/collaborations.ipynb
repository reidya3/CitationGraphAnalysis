{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from fuzzywuzzy import process, fuzz\n",
    "from itertools import combinations\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fuzzywuzzy in c:\\users\\tonyr\\anaconda3\\envs\\py37\\lib\\site-packages (0.18.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborations \n",
    "\n",
    "This notebook details the prepraration of data for the neo4j database.\n",
    "\n",
    "This notebook contains **four** key components:\n",
    "\n",
    "1. Creating methods that will help me gather unique collaborations between a DCU researcher and a DCU researcher/external researcher, the total number of papers collaborated on by that pair and total citation counts achieved by that pair.\n",
    "\n",
    "2. Performing the parsing and creating the dataframe.\n",
    "\n",
    "3. Created a tool that will help me gather unique collaborations between external researchers. Obviously, the total number of papers collaborated on by that pair  and total citation count achieved by that pair  are not available as we do not have their google scholar profiles.\n",
    "\n",
    "4. Creating a CSV file containing unique authors and their job titles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_capitals(l):# adds capitals to author name, cutting down the number of alias's\n",
    "    if l.isupper():\n",
    "        l = l.lower()\n",
    "        l =  l.title()\n",
    "    else:\n",
    "        l = l.title()\n",
    "    return l\n",
    "    \n",
    "\n",
    "def remove_duplicates_stage_1(myList):# removing duplicates and provides the index of the unique authors from the orgianl list\n",
    "    result= []\n",
    "    index = []\n",
    "    marker = set()\n",
    "    for idx,l in enumerate(myList):\n",
    "        ll = l.lower()\n",
    "        if ll not in marker:   # test presence\n",
    "            marker.add(ll)\n",
    "            if l.isupper():\n",
    "                l = l.lower()\n",
    "                l =  l.title()\n",
    "            else:\n",
    "                l = l.title()\n",
    "            index.append(idx)\n",
    "            result.append(l)   # preserve order\n",
    "    return result, index\n",
    "\n",
    "\n",
    "def add_capital_mc(myList):# the add_capitals function removes capitals from any string, if the capital is not at the start.This is a problem for second names that start with mc. e.g. Mccareen becomes McCarren\n",
    "    result=[]\n",
    "    for l in myList:\n",
    "        try:\n",
    "            last_name = l.split(\" \")[-1]\n",
    "            first_name = l.replace(last_name, \"\")\n",
    "            if last_name[:2].lower() == \"mc\":\n",
    "                last_name = last_name[:2] + last_name[2].upper() + last_name[3:]\n",
    "                l = \"{}{}\".format(first_name, last_name).strip()\n",
    "        except IndexError:\n",
    "            pass\n",
    "        result.append(l)\n",
    "    return result\n",
    "\n",
    "def remove_double_author_occurance(flattened_df): # removing occurances where name1 and name2 refer to the same person\n",
    "    for index, name2 in flattened_df[\"Name2\"].items():\n",
    "        name1 = flattened_df[\"Name1\"].loc[index]\n",
    "        score = fuzz.ratio(name1.lower(),name2.lower())\n",
    "        if score >= 90:\n",
    "            flattened_df.drop(index, inplace=True)\n",
    "    flattened_df.reset_index(drop=True, inplace=True)\n",
    "    return flattened_df\n",
    "\n",
    "def check_if_researcher_already_iterated(name, mylist):# the method helps me track the researchers alreaady visted so we don't end up with an occurame of name1:Michael Scriney, name2:Andrew mcCarren  and  name1:Andrew McCarren, name2:Michael Scriney, for example.\n",
    "    if len(mylist) >  1:\n",
    "        score = process.extractOne(str(name.strip()), mylist)[-1]# the ratio score\n",
    "        return score < 90\n",
    "    return True\n",
    "    \n",
    "\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Had to use an excel file as when I tried to use a csv file it kept corrupting special charachters\n",
    "alias_authors = pd.read_excel(\"../data/Neo4j/alias_author_excel.xlsx\") \n",
    "alias_authors[\"Alias\"] = alias_authors[\"Alias\"].apply(lambda x: x.strip())# removing whitespaces\n",
    "alias_authors[\"Author\"] = alias_authors[\"Author\"].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary. Keys are aliases, values are cannon names. \n",
    "alias_dict = dict(zip(alias_authors[\"Alias\"].tolist(), alias_authors[\"Author\"].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_name1 = [] #name1 of our dataframe \n",
    "list_name2 = []# name2 of our datarame\n",
    "list_count = [] # a list ontaining the total citation achieved by a pair of researchers\n",
    "total_paper_count = [] # a list ontaining the total citation count achieved by a pair of researchers\n",
    "researchers_already_iterated = [] # a list cotaining the dcu researchers that we have already visited\n",
    "researchers = pd.read_csv(\"../data/SOC_Researchers.csv\")\n",
    "for name in researchers.Researcher:\n",
    "    name = name.strip() # name of a dcu researcher \n",
    "    filename = \"_\".join(name.split(\" \"))\n",
    "    researcher_citation_count = []\n",
    "    researcher_total_paper = []\n",
    "    try: # checking if a researcher has a google scholar profile \n",
    "        scholar_df = pd.read_csv(\"../data/Google Scholar Publications/{}.csv\".format(filename))\n",
    "        collaborators = [] #list of collaborators for each researcher's\n",
    "        list_researcher = []# list of dcu researchers\n",
    "        for index, authors in scholar_df[\"Author List\"].items():\n",
    "            list_authors = []# a list of authors on a publication\n",
    "            # have to replace diffferent kind of apostrophe to one \n",
    "            for author in authors.split(\", \"):\n",
    "                author = author.replace(\"'\",\"’\").strip()\n",
    "                author = add_capitals(author).strip()\n",
    "                if author in alias_dict:\n",
    "                    author = alias_dict[author]#changing to Cannon name\n",
    "                if check_if_researcher_already_iterated(author, researchers_already_iterated):\n",
    "                    collaborators.append(author)\n",
    "                    list_authors.append(author)\n",
    "            scholar_df.loc[index,\"Author List\"] = \", \".join(list_authors)#changing the scholar dataframe to replace aliases with cannon names\n",
    "            \n",
    "            \n",
    "        for collab in collaborators:\n",
    "            filter_df = scholar_df[scholar_df[\"Author List\"].str.contains(collab)]\n",
    "            researcher_total_paper.append(filter_df.shape[0])#finding total number of papers collaborated on by a pair\n",
    "            researcher_citation_count.append(filter_df['Citation count'].sum()) # finding total citation count collaborated by a pair\n",
    "        collaborators, index_list = remove_duplicates_stage_1(collaborators)\n",
    "        researcher_citation_count = [researcher_citation_count[i] for i in index_list]# removes citation counts achieved by a pair that is duplicated \n",
    "        researcher_total_paper = [researcher_total_paper[i] for i in index_list] #  removes total number of papers achieved by a pair that is duplicated\n",
    "        \n",
    "        collaborators = deepcopy(add_capital_mc(collaborators))\n",
    "        list_researcher = deepcopy([name] * len(collaborators)) \n",
    "        list_name2.extend(collaborators)\n",
    "        list_name1.extend(list_researcher)\n",
    "        list_count.extend(researcher_citation_count)\n",
    "        total_paper_count.extend(researcher_total_paper)\n",
    "        researchers_already_iterated.append(name)           \n",
    "                \n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    \n",
    "d = {\"Name1\":list_name1, \"Name2\": list_name2, \"Total_Paper_Count\": total_paper_count, \"Total_Citations_Achieved\":list_count}\n",
    "flattened_df = pd.DataFrame(data=d)\n",
    "flattened_df = remove_double_author_occurance(flattened_df)# remove occurances where name1 and name2 refer to the same person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The resulting dataframe will have these columns:\n",
    "\n",
    "| Column name | description |\n",
    "|---:|:---|\n",
    "| Name1 | The  first name of a pair that collaborated on at least one paper together. E.g. Michael Scriney |\n",
    "| Name2 | The  second name of a pair that collaborated on at least one paper together. E.g. Andrew McCarren |\n",
    "| Total_Paper_Count | Total number of papers a pair has  collaborated on. |\n",
    "| Total_Citations_Achieved  | Total citation count achieved by a pair.  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colllaborations Between External Researchers\n",
    "Created a tool that will help me gather unique collaborations between external researchers. Obviously, the total number of papers collaborated on  and total citation count achieved by that pair  are not available as we do not have their google scholar profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collaborators = []# a list of the format [[name1, name2], [name1, name2]] etc\n",
    "researchers = pd.read_csv(\"../data/SOC_Researchers.csv\")\n",
    "total_paper_count = [] # a list ontaining the total citation count achieved by a pair of researchers\n",
    "total_citation_count = [] # a list ontaining the total citation count achieved by a pair of researchers\n",
    "dcu_researchers = researchers.Researcher.tolist()# a list of dcu researchers\n",
    "total_collaborations_count = []\n",
    "for name in researchers.Researcher:\n",
    "    name = name.strip() # name of a dcu researcher \n",
    "    filename = \"_\".join(name.split(\" \"))\n",
    "\n",
    "    try: # checking if a researcher has a google scholar profile \n",
    "        scholar_df = pd.read_csv(\"../data/Google Scholar Publications/{}.csv\".format(filename))\n",
    "        for index, authors in scholar_df[\"Author List\"].items():\n",
    "            list_authors = []# a list of authors per publication\n",
    "            # have to replace diffferent kind of apostrophe to one \n",
    "            for author in authors.split(\", \"):\n",
    "                author = author.replace(\"'\",\"’\").strip()\n",
    "                author = add_capitals(author).strip()\n",
    "                if author in alias_dict:\n",
    "                    author = alias_dict[author].strip()\n",
    "                if check_if_researcher_already_iterated(author, dcu_researchers):# removes dcu researchers from list_authors\n",
    "                    list_authors.append(author)\n",
    "            if len(list_authors) > 1:\n",
    "                comb = combinations(list_authors,2) # get all combinations of pairs from list_authors\n",
    "                for i in list(comb):\n",
    "                    collaborators.append(list(i),)\n",
    "        \n",
    "        for collab in collaborators:\n",
    "            filter_1_df = scholar_df[scholar_df[\"Author List\"].str.contains(f\"(?i){collab[0]}\")]\n",
    "            filter_2_df = scholar_df[scholar_df[\"Author List\"].str.contains(f\"(?i){collab[1]}\")]\n",
    "            filter_df = pd.merge(filter_1_df, filter_2_df, how='inner')\n",
    "            total_collaborations_count.append([collab[0], collab[1],filter_df.shape[0], filter_df['Citation count'].sum()])\n",
    "                \n",
    "                \n",
    "    except FileNotFoundError:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes duplicates from thw 2d list regardless of order\n",
    "seen = set()\n",
    "result = []\n",
    "for lst in total_collaborations_count:\n",
    "    current = frozenset(Counter(lst).items())\n",
    "    if current not in seen:\n",
    "        result.append(lst)\n",
    "        seen.add(current)\n",
    "\n",
    "collaborators = result\n",
    "\n",
    "name1_ext, name2_ext, total_paper_count, total_citation_count = [], [], [], []\n",
    "for i in collaborators:\n",
    "    names = add_capital_mc(i[:2])\n",
    "    print(type(names))\n",
    "    print(i)\n",
    "    if i[0] != i[1]:# sometimes the same author name appeared twice  in a publication author list\n",
    "        name1_ext.append(i[0])\n",
    "        name2_ext.append(i[1])\n",
    "        total_paper_count.append(i[2])\n",
    "        total_citation_count.append(i[3])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"Name1\":name1_ext, \"Name2\": name2_ext, \"Total_Paper_Count\": total_paper_count, \"Total_Citations_Achieved\":total_citation_count}\n",
    "extrernal_df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concating the two dafarames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.concat([flattened_df, extrernal_df])\n",
    "total_df = total_df.drop_duplicates(subset=[\"Name1\",\"Name2\"],keep=\"first\",ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique Author Dataframe \n",
    "## The resulting dataframe will have these columns:\n",
    "\n",
    "| Column name | description |\n",
    "|---:|:---|\n",
    "| Unique Authors | A coumn conatining unqiue author names, from all the publications found. |\n",
    "| Job Title | If the author works for DCU, this coulumn displays their job title. Otherwise, unknown. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = total_df[\"Name1\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "name.extend(total_df[\"Name2\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"Unique Author\" :sorted((list(set(name)))), \"Job Title\":\"Unkown\"}\n",
    "unique_df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Michael Scriney\", \"Job Title\"] = \"Assistant Professor\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Malika Bendechache\", \"Job Title\"] = \"Assistant Professor\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Marija Bezbradica\", \"Job Title\"] = \"Assistant Professor\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Stephen Blott\", \"Job Title\"] = \"Associate Professor\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Rob Brennan\", \"Job Title\"] = \"Assistant Professor\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Annalina Caputo\", \"Job Title\"] = \"Assistant Professor\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Long Cheng\", \"Job Title\"] = \"Assistant Professor\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Paul M. Clarke\", \"Job Title\"] = \"Assistant Professor\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Martin Crane\", \"Job Title\"] = \"Associate Professor\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Charlie Daly\", \"Job Title\"] = \"Lecturer\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Brian Davis\", \"Job Title\"] = \"Assistant Professor\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Dónal Fitzpatrick\", \"Job Title\"] = \"Lecturer\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Jennifer Foster\",\"Job Title\"] = \"Lecturer\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Yvette Graham\", \"Job Title\"] = \"Assistant Professor\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Cathal Gurrin\", \"Job Title\"] = \"Associate Professor\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Geoff Hamilton\", \"Job Title\"] = \"Associate Professor\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Graham Healy\", \"Job Title\"] = \"Assistant Professor\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Mark Humphrys\", \"Job Title\"] = \"Assistant Professor\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Musfira Jilani\", \"Job Title\"] = \"Assistant Professor\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Gareth Jones\", \"Job Title\"] = \"Professor\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Jane Kernan\", \"Job Title\"] = \"Lecturer\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Suzanne Little\", \"Job Title\"] = \"Associate Professor\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Silvana Togneri Mac Mahon\", \"Job Title\"] = \"Assistant Professor\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Andrew McCarren\", \"Job Title\"] = \"Assistant Professor\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"John McKenna\", \"Job Title\"] = \"Lecturer\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Alessandra Mileo\", \"Job Title\"] = \"Assistant Professor\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Nivedha Nagarajan\", \"Job Title\"] = \"Teaching Assistant\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Dongyun Nie\", \"Job Title\"] = \"Teaching Assistant\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Mark Roantree\", \"Job Title\"] = \"Professor\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Darragh O'Brien\", \"Job Title\"] = \"Lecturer\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Heather J. Ruskin\", \"Job Title\"] = \"Professor\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Dimitar Shterionov\", \"Job Title\"] = \"Assistant Professor\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"David Sinclair\", \"Job Title\"] = \"Associate Professor\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Alan F. Smeaton\", \"Job Title\"] = \"Professor\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Alistair Sutherland\", \"Job Title\"] = \"Lecturer\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Brian Stone\", \"Job Title\"] = \"Lecturer\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Irina Tal\", \"Job Title\"] = \"Assistant Professor\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Jagadeeswaran Thangaraj\", \"Job Title\"] = \"Teaching Assistant\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] == \"Renaat Verbruggen\", \"Job Title\"] = \"Lecturer\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] ==  \"Ray Walshe\", \"Job Title\"] = \"Assistant Professor\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] ==  \"Monica Ward\", \"Job Title\"] = \"Lecturer\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] ==  \"Tomas Ward\", \"Job Title\"] = \"Professor\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] ==  \"Andy Way\", \"Job Title\"] = \"Professor\"\n",
    "unique_df.loc[unique_df[\"Unique Author\"] ==  \"Murat Yilmaz\", \"Job Title\"] = \"Assistant Professor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_df.to_csv(\"../data/Neo4j/unique_authors.csv\", index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_df.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_id_dict = dict(zip(unique_df[\"Unique Author\"], unique_df.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = total_df.replace({\"Name1\": author_id_dict})\n",
    "total_df = total_df.replace({\"Name2\": author_id_dict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.to_csv(\"collaborations.csv\", index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_df.to_csv(\"unique_authors.csv\", index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
